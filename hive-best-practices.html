<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content=" ">
<title>Hive Integration - Best Practices | Okera User Documentation</title>
<link rel="stylesheet" href="theme/css/syntax.css">
<link rel="icon" type="image/png" href="/assets/images/icon-favicon32.png">

<script src="https://use.fontawesome.com/d9343c4626.js"></script>

<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="theme/css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
  crossorigin="anonymous">
<!-- Old Algolia Instant Search -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css"> -->
<!-- End Old Algolia Instant Search -->
<link rel="stylesheet" href="theme/css/asciidoctor.css">
<link rel="stylesheet" href="theme/css/custom.css">
<link rel="stylesheet" href="theme/css/boxshadowproperties.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="theme/js/jquery.navgoco.min.js"></script>

<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
  crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="theme/js/toc.js"></script>
<script src="theme/js/customscripts.js"></script>

<!-- Old Algolia Instant Search -->
<!-- <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script> -->
<!-- End Old Algolia Instant Search -->

<!-- Include AlgoliaSearch JS Client and autocomplete.js library -->
<script src="https://cdn.jsdelivr.net/algoliasearch/3/algoliasearch.min.js"></script>
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.min.js"></script>

<script>
  $(document).ready(function () {
    $("blockquote:contains('Note')").addClass("admonitionblock note");
    $("blockquote:contains('Warning')").addClass("admonitionblock warning");
    $("blockquote:contains('Tip')").addClass("admonitionblock tip");
    $(".admonitionblock.note").prepend('<i class="fa fa-check admonition"></i>');
    $(".admonitionblock.warning").prepend('<i class="fa fa-exclamation-triangle admonition"></i>');
    $(".admonitionblock.tip").prepend('<i class="fa fa-lightbulb-o admonition"></i>');
  });
</script>


<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->
    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    

</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container topnavlinks">

        <div class="row">
            <div class="col-sm-4 col-md-3">

                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <div class="navbar-wordmark">
                        <a href="/">
                            <img src="assets/images/okera-wordmark-white.svg" alt="Okera wordmark" />
                        </a>
                    </div>
                </div>

                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

                    <ul class="nav navbar-nav">
                        <!-- toggle sidebar button -->
                        <!-- <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li> -->
                        <!-- entries without drop-downs appear here -->

                            
                        <li>
                            <a href="https://okera.com" target="_blank">okera.com</a>
                        </li>
                          
                        <!-- entries with drop-downs appear here -->
                        <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                         
                    </ul>
                </div>
            </div>
            <form class="aa-input-container navbar-form" id="aa-input-container" role="search">
                <div class="form-group">
                    <input id="aa-search-input" type="text" class="aa-input-search form-control" placeholder="Search"
                        autocomplete="off">
                </div>
            </form>
        </div>
        <!-- Algolia Integrated search Script -->
        <script type="text/javascript" src="theme/js/okera-integrated-search.js"></script>
        <!-- comment out this block if you want to hide jekyll search
                  <div class="navbar-form" id="search-demo-container">
                      <form action="/search" method="GET">
                      <input type="text" name="q" id="search-input" placeholder="Search the docs...">
                      </form>
                  </div>
                end search -->
    </div>
</nav>
<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                


<ul id="mysidebar" class="nav affix">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a href="#">Introduction to Okera</a>
      <ul>
          
          
          
          <li><a href="product-overview">Product Overview</a> </li>
          
          
          
          
          
          
          <li><a href="odas-overview">Okera Data Access Service Overview</a> </li>
          
          
          
          
          
          
          <li><a href="catalog-overview">Okera Catalog Overview</a> </li>
          
          
          
          
          
          
          <li><a href="okera-architecture-overview">Platform Architecture Overview</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Installing Okera</a>
      <ul>
          
          
          
          <li><a href="getting-started">Getting Started</a> </li>
          
          
          
          
          
          
          <li><a href="install">Installation Overview</a> </li>
          
          
          
          
          
          
          <li><a href="install-prereqs">Installation Prerequisites</a> </li>
          
          
          
          
          
          
          <li><a href="install-dm">Setting up the Deployment Manager</a> </li>
          
          
          
          
          
          
          <li><a href="install-odas">Setting up the ODAS Cluster</a> </li>
          
          
          
          
          
          
          <li><a href="advanced-install">Advanced Installation Options</a> </li>
          
          
          
          
          
          
          <li><a href="awsguide">AWS Guide</a> </li>
          
          
          
          
          
          
          <li><a href="odas-authentication">Authentication Guide</a> </li>
          
          
          
          
          
          
          <li><a href="kerberos-cluster-setup">Kerberos</a> </li>
          
          
          
          
          
          
          <li><a href="secure-okera-rest-api-ldap">LDAP Authentication</a> </li>
          
          
          
          
          
          
          <li><a href="secure-okera-rest-api-oauth">OAuth Authentication</a> </li>
          
          
          
          
          
          
          <li><a href="secure-deployment-manager-rest-api">Deployment Manager REST Security</a> </li>
          
          
          
          
          
          
          <li><a href="support-versions">Versions</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Administering ODAS Clusters</a>
      <ul>
          
          
          
          <li><a href="cluster-administration">Cluster Administration</a> </li>
          
          
          
          
          
          
          <li><a href="cluster-types">Cluster Types</a> </li>
          
          
          
          
          
          
          <li><a href="standalone-jdbc-cluster">Standalone JDBC Cluster</a> </li>
          
          
          
          
          
          
          <li><a href="cluster-sizing">Cluster Sizing</a> </li>
          
          
          
          
          
          
          <li><a href="memory-usage">Platform Memory Usage</a> </li>
          
          
          
          
          
          
          <li><a href="ip-address-management">IP Address Management</a> </li>
          
          
          
          
          
          
          <li><a href="kubernetes-dashboard-quickstart">Cluster Monitoring with Kubernetes Addons</a> </li>
          
          
          
          
          
          
          <li><a href="cluster-launch-plugin-api">Cluster Launch API</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Integrating with ODAS</a>
      <ul>
          
          
          
          <li><a href="third-party-integration">Integration Overview</a> </li>
          
          
          
          
          
          
          <li><a href="planner-integration">Planner Integration</a> </li>
          
          
          
          
          
          
          <li><a href="pyokera">Native Python Integration</a> </li>
          
          
          
          
          
          
          <li><a href="catalog-rest-api">REST API Integration</a> </li>
          
          
          
          
          
          
          <li><a href="emr-integration">AWS EMR Integration</a> </li>
          
          
          
          
          
          
          <li><a href="odas-cdh-integration">Cloudera CDH Integration</a> </li>
          
          
          
          
          
          
          <li><a href="databricks-integration">Databricks Integration</a> </li>
          
          
          
          
          
          
          <li><a href="client-integration">Hadoop Client Integration</a> </li>
          
          
          
          
          
          
          <li><a href="client-configs">Hive and Spark Client Integration</a> </li>
          
          
          
          
          
          
          <li class="active"><a href="hive-best-practices">Hive Integration - Best Practices</a></li>
          
          
          
          
          
          
          <li><a href="tableau-web-data-connector">Tableau Integration</a> </li>
          
          
          
          
          
          
          <li><a href="aws-cloudtrail-integration">AWS CloudTrail Integration Quick Start</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Using the Catalog</a>
      <ul>
          
          
          
          <li><a href="authorization-abac">Attribute-based Access Control</a> </li>
          
          
          
          
          
          
          <li><a href="authorization">Role-based Access Control</a> </li>
          
          
          
          
          
          
          <li><a href="privileges">Privileges</a> </li>
          
          
          
          
          
          
          <li><a href="schema-design">Schema Design</a> </li>
          
          
          
          
          
          
          <li><a href="authorization-builtins">Authorization Builtin Functions</a> </li>
          
          
          
          
          
          
          <li><a href="auditing">Auditing</a> </li>
          
          
          
          
          
          
          <li><a href="okera-database-cli">Database CLI</a> </li>
          
          
          
          
          
          
          <li><a href="supported-sql">Supported SQL</a> </li>
          
          
          
          
          
          
          <li><a href="jdbc-data-source">JDBC Data Source</a> </li>
          
          
          
          
          
          
          <li><a href="data-types">Supported Data Types</a> </li>
          
          
          
          
          
          
          <li><a href="complex-types">Complex Data Types</a> </li>
          
          
          
          
          
          
          <li><a href="extending-odas">Extending ODAS</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Using the Web UI</a>
      <ul>
          
          
          
          <li><a href="web-ui-basics">Okera Portal Basics</a> </li>
          
          
          
          
          
          
          <li><a href="datasets-page">Datasets Page</a> </li>
          
          
          
          
          
          
          <li><a href="permissions-page">Permissions Page</a> </li>
          
          
          
          
          
          
          <li><a href="workspace-page">Workspace</a> </li>
          
          
          
          
          
          
          <li><a href="reports-page">Reports Page</a> </li>
          
          
          
          
          
          
          <li><a href="data-registration">Data Registration</a> </li>
          
          
          
          
          
          
          <li><a href="managing-tags">Managing Tags</a> </li>
          
          
          
          
          
          
          <li><a href="autotagging-configuration">Configuring Auto-Tagger</a> </li>
          
          
          
          
          
          
          <li><a href="web-ui-admin">Okera Portal for Admins</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Reference</a>
      <ul>
          
          
          
          <li><a href="release-notes">Release Notes</a> </li>
          
          
          
          
          
          
          <li><a href="earlier-release-notes">Earlier Release Notes</a> </li>
          
          
          
          
          
          
          <li><a href="service-port-assignments">Service Ports</a> </li>
          
          
          
          
          
          
          <li><a href="s3-encryption-support">Supported S3 Encryption Types</a> </li>
          
          
          
          
          
          
          <li><a href="compatibility-guarantees">Compatibility Guarantees</a> </li>
          
          
          
          
          
          
          <li><a href="quick-recipes">Quick Recipes</a> </li>
          
          
          
          
          
          
          <li><a href="https://okera.zendesk.com/hc/en-us/categories/115000493607-FAQs" target="_blank">Frequently Asked Questions</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#">Tutorials</a>
      <ul>
          
          
          
          <li><a href="onboarding-datasets">Onboarding datasets</a> </li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
        -->
    
    <li>
    
<!-- this handles the automatic toc. use ## for subheads to auto-generate the on-page minitoc. if you use html tags, you must supply an ID for the heading element in order for it to appear in the minitoc. -->
<script>
$( document ).ready(function() {
  // Handler for .ready() called.

$('#toc').toc({ minimumHeaders: 0, listType: 'ul', showSpeed: 0, headers: 'h2', title: 'On This Page' });

/* this offset helps account for the space taken up by the floating toolbar. */
$('#toc').on('click', 'a', function() {
  var target = $(this.getAttribute('href'))
    , scroll_target = target.offset().top

  $(window).scrollTop(scroll_target - 80);
  return false
})

});
</script>

<div id="toc"></div>

    </li>
    

</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>

            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            

<div class="post-content">

   

    


    

  <h1 id="hive-integration---best-practices">Hive Integration - Best Practices</h1>

<h2 id="goals-and-audience">Goals and Audience</h2>

<p>This document summarizes the information contained in the online documentation pertaining the use of Hive with the Okera Platform.
It links the existing documents where possible and otherwise adds a consistent narrative to what Okera recommends when used with Hive.
The target audience is architects, developers, and database administrators (DBAs).</p>

<h2 id="introduction">Introduction</h2>

<p>Apache Hive is a library that converts SQL (to be precise, <em>Hive SQL</em>) into processing jobs that can be executed by various Hadoop-provided backends.
This includes Apache YARN for batch processing, and Apache Tez for more ad-hoc type of queries.
The process of doing contains the following steps:</p>

<ul>
  <li>HiveQL is submitted to Hive by the user (using, for example, the Hive CLI)</li>
  <li>The Hive client parses the query and plans the request, which includes:
    <ul>
      <li>Contacting the Hive Metastore to get the metadata for all contained tables and views in the query</li>
      <li>Computing a plan that expresses the query actions (such as aggregations, filters, and data scans)</li>
      <li>Optimizing the query based on (optionally) available statistics</li>
      <li>Determining resource requirements, based on data locality and query parallelism</li>
    </ul>
  </li>
  <li>With the plan completed, the Hive client starts a job with those resource requirements in the configured processing backend</li>
  <li>The processing starts within the framework resulting in one or more stages of the query actions to run concurrently</li>
  <li>Intermittent (as in temporary) data is swapped to disk or cached in memory, dependent on the engine that runs the query</li>
  <li>All resulting data is persisted or streamed to the client for final delivery</li>
</ul>

<p>While some of these steps in the planning phase are very similar to what is done by an RDBMS when executing SQL queries, there is quite a difference in the execution phase.
This manifests itself in the need to understand the underlying engine and how its stage execution can affect the overall query performance.</p>

<p>This is no different than running a well- or badly written MapReduce or Spark job: when the job is not using the framework efficiently, the overall outcome is suboptimal.
For this reason, using Hive mainly revolves around writing queries in such a way that it performs as expected.
This includes making a conscious decision about:</p>

<ul>
  <li>
    <p><strong>Data Types</strong> - This is akin to regular databases, as in not to use costly types like <code class="highlighter-rouge">STRING</code> in favor of numeric types where possible.</p>
  </li>
  <li>
    <p><strong>Partitioning</strong> - Large file system-backed datasets <em>must</em> be partitioned by one or more column value, for example a date or geographical region, to ensure the underlying data is grouped into smaller files.
In combination with specific SQL <code class="highlighter-rouge">WHERE</code> clauses this enables more efficient querying by eliminating unnecessary data files altogether, referred to as <em>partition pruning</em>.
The number of partitions for a table should be not too high, that is, in excess of 100,000 partitions.
It should also not be too low and, for example, cause each partition to contain tenths of gigabyte of data or more.
The goal is to have a few hundred megabyte per partition.
For date based partitioning, in most cases, day level partition works very well.</p>
  </li>
  <li>
    <p><strong>File Formats</strong> - It is recommended to use a binary, block-based file format (when using a file system backend such as HDFS or S3).
Depending on the use-case the choice in practice is between Avro (for row based access use-cases, such as batch oriented full dataset scans) or Parquet (for column-based use-cases, such as highly selective analytics).
Text-based file formats are not ideal as they require more resources for parsing the text into binary objects.</p>
  </li>
  <li>
    <p><strong>Compression</strong> - Most datasets contain data that compresses well, reducing the necessary I/O operations needed to read the data.
The extra cost of decompression is often negligible as systems are bound by other low-level resources first (such as networking or storage I/O).
If the data is not already compressed, enabling block-based compression for the above file formats is recommended as a default.
For processing engines that spill intermediate data to disk it is also recommended to enable data compression as well as a suitable binary file format.</p>
  </li>
  <li>
    <p><strong>Block Size</strong> - A common optimization for large-scale data processing is reading larger chunks of data from the same execution task.
One reason is that starting up executors is a relatively slow operation (especially for YARN-backed processing), and another is that I/O is faster if data is transferred in contiguous chunks of reasonable size, referred to as <em>streaming reads</em>.
For example, a block size could be set to 256MB and is read in chunks of 16KB.</p>
  </li>
  <li>
    <p><strong>Combine/Split Files</strong> - Engines that read data from files can be configured to combine smaller or split larger files to even out the perceived block sizes from the executor tasks.</p>
  </li>
</ul>

<p>In general, these decision points are the same <em>with</em> Okera, since the ODAS Workers perform the same operations for I/O.</p>

<h2 id="impact-of-okera">Impact of Okera</h2>

<p>Adding the Okera Platform in many ways does not change this pattern.
In fact, you can think of Okera’s ODAS as another table (or view) on top of raw data.
Complex queries that contain analytical functionality, such as the use of <code class="highlighter-rouge">DISTINCT</code> and <code class="highlighter-rouge">GROUP BY</code>, are <a href="supported-sql#data-manipulation-language-dml-statements">neither allowed nor supported</a>, in Okera.
Instead, Okera uses SQL DML <em>only</em> to enable efficient access control expressed in a common syntax.</p>

<p>The greatest impact of introducing Okera to an existing Hive stack is the change in <a href="schema-design">schema design</a> and the setup of the used <a href="#hive-metastore">Hive Metastore</a> for the Okera Schema Registry.</p>

<h3 id="schema-design">Schema Design</h3>

<p>Okera recommends the use of a layered schema design, which mitigates schema drift issues of the raw data source by decoupling the users from those resources.
The layers are (explained from the bottom to the top):</p>

<ol>
  <li>
    <p><strong>Raw Tables</strong> - All registered datasets are represented as-is by a TABLE object.
This means the original schema of the underlying data source is mapped into the Okera Schema Registry without any modification.</p>
  </li>
  <li>
    <p><strong>Cleanup Views</strong> - Any structural modifications are covered by this layer.
The views wrap the raw tables one-to-one but are used to rename fields, cast types to appropriate ones, and fix column values (for example, remove a technical prefix that is obsolete).
No filtering through the use of <code class="highlighter-rouge">WHERE</code> clauses or combining of tables is allowed. The cleaned views form the entry level object layer for data consumers.
See <a href="#schema-evolution">Schema Evolution</a> for more information about how this layer helps to protect users from changes.</p>
  </li>
  <li>
    <p><strong>Access Control Views</strong> - This layer adds any access related processing, including the use of functions to mask or tokenize column values, filtering using <code class="highlighter-rouge">WHERE</code> clauses (for example, filter all rows based on geography) or <code class="highlighter-rouge">JOIN</code> statements (for instance, filter records using a blacklist with an <code class="highlighter-rouge">ANTI JOIN</code>).
Access control is not needed for all datasets, making this layer optional.
For example, <em>public</em> datasets that can be accessed by all authenticated users do not need an access control view.</p>
  </li>
  <li>
    <p><strong>External Views</strong> - For any query that contains unsupported SQL functionality, such as aggregations for analytical processing, the <code class="highlighter-rouge">EXTERNAL</code> keyword can be used during the definition of a VIEW to cause its evaluation to occur in the downstream processing engine (which is not just Hive, but also Spark and similar tools). See the <a href="#external-views">External Views</a> section for details.</p>
  </li>
</ol>

<p><img src="assets/images/schema-design-extended.png" alt="Schema Design" /></p>

<p>Defining those views over the base table is not costly, since the Okera Platform is evaluating them at query plan time.
In other words, the operations of each view over the base table are combined into a single SQL statement, complete with user role-dependent actions.
When the query is executed on the ODAS Workers, there is no further cost to the slightly more involved schema hierarchy.
Essentially, it is free of any overhead.</p>

<p>With this approach you have a schema design architecture that accomplishes the following:</p>

<ul>
  <li>Protects from schema evolution of the ingested data</li>
  <li>Allows to cleanup of raw table structures</li>
  <li>Enables efficient and comprehensive access control</li>
  <li>Leaves the execution of compute-intense operations to downstream engines</li>
  <li>Avoids adding overhead during query execution time</li>
</ul>

<p>Okera recommends to follow those <a href="schema-design">guidelines</a> to provide for reliable and future-proof metadata management.</p>

<h4 id="schema-evolution">Schema Evolution</h4>

<p>Should the structure of the external data source change (that is, the raw table schema changes), adjustments can be made in the cleanup views.
That way, the raw table structure is disconnected from the users, so that low-level changes are only reflected in the view definition: All downstream consumers can use the view as they did before, avoiding the need to react to any schema change immediately.</p>

<p>Of course, there may be so substantial changes in the raw table structure that the view is eventually not able to disguise them.
In that case a new <code class="highlighter-rouge">VIEW</code> could be defined with a new name, representing the way forward.
Those who need to get access to the latest schema can migrate to the new view at their own convenience.</p>

<h4 id="external-views">External Views</h4>

<p>The way VIEWs are defined changes how they are evaluated.
For example, consider this <code class="highlighter-rouge">VIEW</code> definition:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE VIEW sales_transactions_cal
AS SELECT &lt;fields&gt; ...
FROM sales_transactions
WHERE region = 'california'
</code></pre></div></div>

<blockquote>
  <p><strong>Note:</strong> The base <code class="highlighter-rouge">sales_transactions</code> dataset is assumed to be an access control view.</p>
</blockquote>

<p>Usually when a user query is containing such a view, for instance</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT * FROM sales_transactions_cal
</code></pre></div></div>

<p>the ODAS Planner returns the <code class="highlighter-rouge">VIEW</code> as a <code class="highlighter-rouge">TABLE</code> definition (that is, it returns the fields in the final schema) when asked by the compute framework.
This can be emulated by using the <code class="highlighter-rouge">SHOW CREATE TABLE</code> command from the Hive CLI (ignoring the remainder of the output):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hive&gt; SHOW CREATE TABLE sales_transactions_cal;
OK
CREATE EXTERNAL TABLE sales_transactions_cal(
  txnid bigint COMMENT '',
  dt_time string COMMENT '',
  sku string COMMENT '',
  userid int COMMENT '',
  price float COMMENT '',
  creditcard string COMMENT '',
  ip string COMMENT '')
...
</code></pre></div></div>

<p>This leaves the framework no choice but read from that (pseudo) table like it is a raw source of data (such as an S3 file).</p>

<p>Okera supports the <a href="supported-sql#creating-external-views"><code class="highlighter-rouge">EXTERNAL</code></a> keyword to define a view that is different from this default behavior.
For example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE EXTERNAL VIEW revenue_cal
AS SELECT min(revenue) as minRevenue, max(revenue) as maxRevenue
FROM sales_transactions
WHERE region = 'california'
</code></pre></div></div>

<p>The view uses an aggregation function that is not supported by Okera itself.
The <code class="highlighter-rouge">EXTERNAL</code> keyword changes how the ODAS Planner is returning the <code class="highlighter-rouge">VIEW</code> definition when asked by the compute framework, returning the full <code class="highlighter-rouge">VIEW</code> definition instead:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hive&gt; SHOW CREATE TABLE revenue_cal;
OK
CREATE VIEW revenue_cal AS SELECT * FROM salesdb.sales_transactions
WHERE region = 'california'
</code></pre></div></div>

<p>This causes the evaluation of the <code class="highlighter-rouge">VIEW</code> outside of Okera, and inside the downstream compute framework instead.
All of the unsupported SQL is handled downstream as well, while the source of the <code class="highlighter-rouge">VIEW</code>, here the <code class="highlighter-rouge">sales_transaction</code> access control view, is still handled by Okera, applying all the filtering and audit logging as expected.</p>

<h4 id="default-partition-pruning">Default Partition Pruning</h4>

<p>DBAs have even more options using layered views in Okera, allowing them, for example, to limit the amount of data returned from a partitioned table when no matching WHERE clauses was defined.
The <a href="supported-sql#last-partition"><code class="highlighter-rouge">LAST PARTITION</code></a> feature can avoid unnecessary full table scans (along the lines of <code class="highlighter-rouge">SELECT * FROM &lt;table_or_view&gt;</code> to check what data a dataset holds, just to realize it has terabytes or petabytes of data) that may inadvertently choke up valuable system resources.</p>

<p>This feature is very flexible and (as of Okera 1.2) allows to specify how many data files or how many partitions should be read instead of only the very last one.
For example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE VIEW sampledb.ymdata_recent AS
SELECT * FROM sampledb.ymdata(LAST PARTITION);

CREATE VIEW sampledb.ymdata_last_file AS
SELECT * FROM sampledb.sample(LAST 1 FILES);

CREATE VIEW sampledb.ymdata_last_4_months AS
SELECT * FROM sampledb.ymdata(LAST 4 PARTITIONS);
</code></pre></div></div>

<p>This is also applicable to the raw tables, using <em>table properties</em>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ALTER TABLE sampledb.tbl_ds1_last_1 SET TBLPROPERTIES(
  'okera.default-last-n-partitions'='1')
</code></pre></div></div>

<p>Okera recommends using the default partition pruning on large tables with more than 100 partitions.</p>

<h4 id="writing-data">Writing Data</h4>

<p>Hive supports multiple SQL statements to write data back into datasets. These are:</p>

<ul>
  <li>
    <p><strong><code class="highlighter-rouge">CREATE ... TABLE ... &lt;table&gt; ... AS SELECT ... FROM ...</code></strong></p>

    <p>With one statement the user can create a new table and fill it with data returned by the specified query.</p>
  </li>
  <li>
    <p><strong><code class="highlighter-rouge">INSERT OVERWRITE TABLE &lt;table&gt; ... SELECT ... FROM ...</code></strong></p>

    <p>This statement can be used to replace a partition or entire table with data generated by a given query.</p>
  </li>
  <li>
    <p><strong><code class="highlighter-rouge">INSERT INTO TABLE &lt;table&gt; ... SELECT ... FROM ...</code></strong></p>

    <p>Instead of replacing, this statement will append the data from the specified query to the named table or partition.</p>
  </li>
  <li>
    <p><strong><code class="highlighter-rouge">INSERT INTO TABLE &lt;table&gt; ... VALUES ...</code></strong></p>

    <p>This statement is the same as provided by regular RDBMSs, where concrete values are inserted into an existing table.
  In practice, this is used mostly to generate (small) test data.</p>
  </li>
</ul>

<p>All of these queries share the need to access the underlying storage location of the dataset.
With Okera, you have two choices of handling these types of operations:</p>

<ol>
  <li>
    <p>Use a <a href="#local-filesystems">Local Filesystem</a></p>

    <p>In this case all writes bypass Okera altogether.
 This is useful for sandboxing or temporary staging of data on compute clusters, using a cluster-local filesystem, such as HDFS on ephemeral disks attached to the virtual machines.
 It is ideally suited for temporary datasets which have the same lifetime as the compute cluster.</p>
  </li>
  <li>
    <p>Use a Shared Filesystem</p>

    <p>Here the user (which could be a real user or a technical user that executes a data pipeline) runs Hive queries that write data into the shared filesystem, such as AWS S3 or Azure Blob Storage (WASB).</p>
  </li>
</ol>

<blockquote>
  <p><strong>Notes:</strong></p>
  <ul>
    <li>For either choice you need to have write access to the underlying storage system(s).</li>
    <li>For option #2, you also <strong>must</strong> have been ALL or INSERT permissions granted in the Okera Policy Engine.</li>
  </ul>
</blockquote>

<p>For shared filesystems, there two modes of operations that are supported, which is controlled by setting the boolean configuration variable
<code class="highlighter-rouge">okera.hive.allow-original-metadata-on-all-tables</code> to either <code class="highlighter-rouge">true</code> or <code class="highlighter-rouge">false</code> (in the <code class="highlighter-rouge">hive-site.xml</code> file), with <code class="highlighter-rouge">false</code> being the default:</p>

<ul>
  <li>
    <p>Allow Original Metadata (Variable set to <code class="highlighter-rouge">true</code>)</p>

    <p>When this setting is chosen, Okera is enabling writes to partitioned and non-partitioned tables.
 Conversely, this exposes the underlying metadata of non-partitioned tables to clients, which disables their fine-grained access control.
 Partitioned tables are still fully controlled for reads.</p>
  </li>
  <li>
    <p>Do <em>Not</em> Allow Original Metadata (Variable set to <code class="highlighter-rouge">false</code>)</p>

    <p>The default mode does <em>not</em> expose the metadata for non-partitioned tables, meaning they cannot be written to.
 Conversely, fine-grained access control for reads are enabled.
 For partitioned tables, both read and writes are possible.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note:</strong> Okera currently <em>only</em> supports writing to file systems (based on Hadoop’s <code class="highlighter-rouge">FileSystem</code> implementation).
In other words, JDBC writing is <em>not</em> supported.</p>
</blockquote>

<p>The following table summarizes the read and write support for either mode.</p>

<table>
  <thead>
    <tr>
      <th>Mode</th>
      <th>Partitioned Table</th>
      <th>Non-Partitioned Table</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">true</code></td>
      <td>Read &amp; Write</td>
      <td>Write</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">false</code> (default)</td>
      <td>Read &amp; Write</td>
      <td>Read</td>
    </tr>
  </tbody>
</table>

<p>Depending on the existence of both table types, or only one of them, the administrator is able to choose the required mode when writes are necessary.</p>

<p>In practice, the reason to read and/or write data is manifold.
The following diagram shows a common scenario, where <em>data producers</em> write authoritative (full fidelity) data into the storage systems.
These are usually privileged processes that run as part of automated data pipelines.
As such, they have elevated rights to read and write directly from and to the storage systems.</p>

<p>On the other hand, <em>data consumers</em> are running their own compute resources to analyse the data.
Reading the data is fully controlled by the Okera Policy Engine, ensuring fine-grained access control is enforced as expected.
Consumers can still write data into storage systems, for example persisting intermediate or resulting data, but do so with regular permissions and into separate storage locations.</p>

<p><img src="assets/images/write-arch.png" alt="Writes in the context of ODAS" title="Writes are handled out-of-band using ODAS" /></p>

<h3 id="hive-metastore">Hive Metastore</h3>

<p>Okera allows for flexible setups regarding the Hive Metastore service.
The following are common scenarios in practice:</p>

<ul>
  <li>
    <p><strong>Embedded HMS</strong> - When you set up the Okera Platform, the default settings assume that Okera is responsible to run and maintain the metastore, which drives the Schema Registry.</p>
  </li>
  <li>
    <p><strong>External HMS</strong> - Another option is to use an existing Hive Metastore, particularly in Hadoop environments which have been configured with their own instance of HMS.</p>
  </li>
</ul>

<p>When configuring an external <a href="advanced-install#special-hive-metastore-management">Hive Metastore</a>, Okera will be very careful not to interfere with the existing database that is backing the existing metastore.
Worst case scenario is that Okera is not able to read the schema of the metastore database and fail to start.
If that happens, contact <a href="http://okera.com">Okera</a> to discuss support options.</p>

<h2 id="hive-configuration">Hive Configuration</h2>

<p>The online <a href="client-configs#hive">client documentation</a> lists the possible configuration settings for Hive.
The <a href="#troubleshooting">troubleshooting</a> section below uses some of those settings to address common issues.</p>

<blockquote>
  <p><strong>Note:</strong> You must enable the Hive client integration provided by Okera to get fully seamless functionality with an ODAS cluster.</p>
</blockquote>

<h3 id="distributed-filesystems">Distributed Filesystems</h3>

<p>For cloud-based Hive setups, such as AWS EMR, Okera recommends to use the accompanying distributed file- or object service, that is, S3.
In general, you need to ensure data is kept persistent and is not stored in ephemeral storage like, for example, EC2 instance storage, which is deleted when the compute cluster is deleted.</p>

<p>It is also recommended to configure the persistent storage system as the default Hive <em>warehouse</em> directory.
This can be set in the <code class="highlighter-rouge">hive-site.xml</code> configuration file.</p>

<p><strong>Example:</strong> Changing the default table locations to an S3 bucket</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hive.metastore.warehouse.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>s3://<span class="nt">&lt;bucket&gt;</span>/warehouse<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>

<h3 id="local-filesystems">Local Filesystems</h3>

<p>For Hadoop clusters, such as AWS EMR, there may be a local filesystem available (usually HDFS) that can be used for specific purposes.
For example, the <em>cluster-local</em> filesystem can be used to experiment with data and processing jobs that completely bypass Okera.
This is recommended by Okera for the development of <em>user-defined functions</em> (UDFs), since it does not require any premature registration with the Okera Platform and allows for easy replacement of the resources (that is, the Java JARs).</p>

<blockquote>
  <p><strong>Note:</strong> Since the Okera Platform is bypassed, no access control and audit event logging is performed.
Only use the cluster-local filesystem for non-sensitive data (such as anonymized test data), or ensure that the cluster is otherwise properly secured.</p>
</blockquote>

<p>See the <a href="emr-integration#cluster-local-databases">EMR documentation</a> and <a href="authorization#developing-functions">Developing Functions</a> for details.</p>

<h2 id="troubleshooting">Troubleshooting</h2>

<p>In practice, adding Okera to an existing environment changes some of the behavior of queries.
Listed below are some known issues.</p>

<h3 id="planner-timeouts">Planner Timeouts</h3>

<p>For file system-based datasets, if no default <a href="#default-partition-pruning">partition pruning</a> is set and a user is running a full table scan (for example using <code class="highlighter-rouge">SELECT * FROM &lt;table_name&gt;</code>, the ODAS Planner needs to check the metadata of any data file that is part of the query.
Large tables may have many files and slower file systems for metadata operations, such as S3, will require more time than what is configured as a timeout between the client and the ODAS Planer.
This leads inevitably to problems where queries timeout during the planning phase.</p>

<p>Setting the timeout higher can be accomplished by using the following configuration property:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>recordservice.planner.rpc.timeoutMs=300000 # 5 mins in milliseconds
</code></pre></div></div>

<p>See the <a href="https://okera.zendesk.com/hc/en-us/articles/360005824214">FAQ article</a> for more details.</p>

<h3 id="worker-timeouts">Worker Timeouts</h3>

<p>For batch jobs the processing may require a considerable amount of time, causing the connection between the client and the ODAS Worker nodes to time out.</p>

<p>The following property can be used to set the timeout higher:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>recordservice.worker.rpc.timeoutMs=1800000 # 30mins in milliseconds
</code></pre></div></div>


    <div class="tags">
        
    </div>




</div>



<footer>
  <div class="row">
    <div class="col-lg-12 footer">
      <div class="footer-meta">
        &copy;2019 Okera, Inc. All rights reserved.  Site last generated: May 28, 2019
      </div>
    </div>
  </div>
</footer>

        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

</html>
