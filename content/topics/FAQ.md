# Frequently Asked Questions about the Okera Platform

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:0 orderedList:0 -->
<!-- TOC Automatically generated by markdown-toc Atom plugin -->
- [Basic Product Questions](#basic-product-questions)
  - [What is Okera?](#what-is-okera)
  - [Why do I need Okera?](#why-do-i-need-okera)
  - [Is Okera a database?](#is-okera-a-database)
  - [Where does Okera fit in my infrastructure stack?](#where-does-okera-fit-in-my-infrastructure-stack)
  - [Does Okera colocate with my existing Hadoop clusters?](#does-okera-colocate-with-my-existing-hadoop-clusters)
  - [How does Okera impact performance?](#how-does-okera-impact-performance)
  - [Do I still need to prep the data if I have Okera?](#do-i-still-need-to-prep-the-data-if-i-have-okera)
  - [Do I still need a data catalog if I have Okera?](#do-i-still-need-a-data-catalog-if-i-have-okera)
  - [Once set up, will all data access be through Okera?](#once-set-up-will-all-data-access-be-through-okera)
  - [How do I get support?](#how-do-i-get-support)
  - [How does Okera handle identity management and authentication?](#how-does-okera-handle-identity-management-and-authentication)
  - [Is Okera Case Sensitive?](#is-okera-case-sensitive)
  - [Does Okera support table statistics?](#does-okera-support-table-statistics)

- [Technology Questions](#technology-questions)
  - [What data formats does Okera support?](#what-data-formats-does-okera-support)
  - [Does Okera support any user-defined functions (UDFs)?](#does-okera-support-any-user-defined-functions-udfs)
  - [Does Okera use YARN?](#does-okera-use-yarn)
  - [How does Okera route front-end requests?](#how-does-okera-route-front-end-requests)
  - [How does Okera solve the small-files problem?](#how-does-okera-solve-the-small-files-problem)
  - [Can Okera Catalog span multiple data centers?](#can-okera-catalog-span-multiple-data-centers)
  - [Does Okera support any type of data indexing?](#does-okera-support-any-type-of-data-indexing)
  - [Does Okera guarantee backward compatibility with Amazon EMR?](#does-okera-guarantee-backward-compatibility-with-amazon-emr)

- [Usage Questions](#usage-questions)
  - [Are there differences between MSCK REPAIR TABLE in Hive vs ALTER TABLE RECOVER PARTITIONS in ODAS?](#are-there-differences-between-msck-repair-table-in-hive-vs-alter-table-recover-partitions-in-odas)
  - [What exactly is granting ALL on a URI needed for?](#what-exactly-is-granting-all-on-a-uri-needed-for)
  - [What is the largest scale Okera has run at?](#what-is-the-largest-scale-Okera-has-run-at)
  - [Implicit Conversions- Datatype not comparable?](#implicit-conversions-datatype-not-comparable)
  - [Lateral Views in Okera](#lateral-views-in-okera)
  - [Does a schema order matter for parquet files?](#does-a-schema-order-matter-for-parquet-files)
  - [Is creating a table using XML SerDes supported?](#is-creating-a-table-using-xml-serdes-supported)
  - [Is time to deploy dependent on cluster size?](#is-time-to-deploy-dependent-on-cluster-size)
  - [Recover vs Add Partitions](#recover-vs-add-partitions)
  - [Auto Partition Recovery](#auto-partition-recovery)
  - [Granting on URI](#granting-on-uri)

<!-- /TOC -->

## Basic Product Questions

### What is Okera?

Okera solves the complex issue of managing and controlling access to many big data datasources.
It transparently integrates into Hadoop, S3, EMR, and Spark (and many more architectures!) to provide a unified metadata catalog and access control policy store.
Data access, as well as policy access and enforcement, are managed by Okera services.

Like massively parallel processing (MPP) databases, Okera is always available, taking on requests from a growing set of data processing systems, such as Spark, Python, Presto, Impala, and Hive, with no delay and no additional overhead.
Its scalability and high availability features make Okera the ideal backend for all access to data in many of the common storage systems, like S3, HDFS, and Kafka.
Such storage tools often only provide coarse-grained security controls.
With Okera, all access to data is vetted against the central access control policies and logged for auditing.
Because of Okera's optimized access to data using state-of-the-art technologies, queries are accelerated and can take further advantage of acceleration mechanisms provided by Okera.

Through the integration with industry-standard security mechanisms such as Kerberos authentication protocol and Microsoft Active Directory servers, access to data is authenticated against the existing company-wide user management systems.

All of this ensures that Okera is not introducing any additional complexity, but rather simplifies your life and makes access to data a reliable, secure, and transparent experience.

For more, see [Okera Platform Overview](product-overview).

### Why do I need Okera?

We live in a time when technology and data are exploding exponentially, making their integration more and more complicated.
At the same time, users are asking for easier access to both, in an attempt to focus on business problems, without having to learn yet another interface.
On top of that, providing secure access to the data is not trivial and certainly not easy; each technology has its own way of securing data, and where that mechanism is delegated to a shared service, it may only offer only coarse-grained access.

This is where Okera comes in: combining disjointed APIs into one unified system that offers a central schema catalog, access policy store, and shared data access layer.
Using Okera, all metadata and data access is simplified, as the unnecessary need to add access policies to many systems is removed.
Fine-grained control over data down to the object, row, and a column, enables users to access what they need seamlessly.
In addition, Okera has the ability to speed up common access patterns and thereby increase the effectiveness of applications, increasing the return on investment.

### Is Okera a database?

Okera manages data access and governance, not storage, per se.
Yet as a matter of course, the platform does provide much of the functionality you may expect from conventional databases but is not found in Big Data infrastructure.
Okera maintains a schema registry that maps datasources, objects, and fields similarly to some features of relational database management systems (RDBMSes).
See [Okera Catalog Overview](catalog-overview) for more.

### Where does Okera fit in my infrastructure stack?

Okera provides access to underlying data sources.
It offers functionality for end-user query access to data using various applications (business intelligence tools) or programmatically (such as by Apache Spark or Python scripts).

Typically the provisioning of data to the underlying data stores (for example, S3 or HDFS) is carried out by common ingest pipelines, which have full and direct access to the data sources and write access to the storage infrastructure.
That is, you ingest, organize, and store your data how you see fit.

An instance of the Okera platform takes the form of a host cluster that has access to your data sources.
All your key applications have access to your data over [ODAS](odas-overview), according to schemas and rules you impose using [Okera Catalog](catalog-overview).

See [Okera Data Access Service Overview](odas-overview) for more.

### Does Okera colocate with my existing Hadoop clusters?

Okera can be installed collocated with your current Hadoop infrastructure or separately.
Both models are fully supported.
Which model you pick depends on your environment and needs.

* If you're deploying in the data center where all services are collocated on a set of physical machines, Okera can be deployed on the cluster, collocated with the other services.

* If you're deploying in the cloud and have different kinds of access frameworks (Spark, Presto, Python) accessing data via Okera but not necessarily all collocated on the same machines, Okera can be deployed standalone as an independent service.

* With Amazon's Elastic MapReduce (EMR), you can deploy Okera collocated with the EMR nodes.

### How does Okera impact performance?

Okera has been designed with performance in mind.

* The I/O and data path are tuned to be highly efficient.
* Clusters have been proven to scale to 800+ nodes.

Okera deployment supports federation (deploying multiple clusters with hundreds of nodes).
In many workloads, using the most-popular Hadoop analytic frameworks (Spark, MapReduce), performance often improves between 10-20% due to the data path efficiency.
One way to think about this is that the MapReduce/Java application offloads the initial processing to Okera, including I/O, file parsing, decompression, and filtering.

### Do I still need to prep the data if I have Okera?

Yes.
Data prep involves modeling the data to suit your business and transforming data needed in the process of ingesting and storing it.
Once modeled, data can be published with Okera.
Okera thereafter provides self-service access to your business users, analysts, and scientists, all using their favorite utilities, apps, scripting languages like Python and R.

### Do I still need a data catalog if I have Okera?

Okera provides you a Catalog for what we call “technical metadata”.
This is the information required to access the data sets: schemas, location, access policies.
For some users, this is sufficient.
For others, we've seen them build a catalog that is designed to more specifically model their business objects.
In this case, the system would work with the Okera Catalog.
Okera Catalog could contain the lower-level information, but the catalog would contain higher-level information associated with specific business processes.

To be explicit, customers that are used to running Hive Metastore or Sentry Store do not need to do so anymore.

### Once set up, will all data access be through Okera?

There is no strict requirement to have Okera manage all data access.
The platform can be configured such that certain users can go directly to the raw data.
Anything that accesses raw files directly will be unable to leverage the security and governance functionality the Okera platform provides.

### How does Okera handle identity management and authentication?

Okera honors user groups established integrated by Unix systems and Active Directory.
When users supply authentication credentials the Okera platform, their existing authorizations are applied, permitting them access only to the datasets, objects, and columns indicated by their group assignments.

### How do I get support?

Okera Customer Success engineers provide excellent enterprise support services under the terms of our license.
To become a valued Okera customer, [get in touch](http://okera.com).
We do not guarantee support to non-customers, but we [welcome support inquiries](http://okera.com/contact).

If you are already an Okera customer, you or a representative at your organization should have a [ZenDesk support login](http://support.okera.com/hc/en-us).

## Technology Questions

### What data and file formats does Okera support?

File formats are distinguished by two types of format:

- The enclosing container format of the file.
- The record serialization format within the file.

For example, you can have a text based file that contains *comma-separated values* (CSV), or JSON objects.
Or you could have a binary file in Hadoop SequenceFile container format, containing records serialized using Thrift.

Okera support pluggable serialization formats for some of the container formats.
For example, for text-based files you can extend Okera using the common [Hive SerDe](extending-odas#serdes) functionality.

The following table lists the supported file formats and their features:

| Container | Type | Boundaries | Serialization | Extensible | Supported | Compressible | Splittable |
| --------- | :---: | :--------: | ------------- | :--------: | :-------: | :----------: | :--------: |
| Text | Text | Lines | Text, CSV, TSV, JSON, RegEx | Yes (Hive SerDes) | Yes | External | Yes (uncompressed only) |
| Parquet | Binary | Blocks | Columnar | No | Yes | Yes (Block) | Yes (Block) |
| SequenceFile | Binary | Blocks | Key/Value Pairs | No | Limited (`Text` values only) | Yes (Block and Record) | Yes (Block) |
| RCFile | Binary | Blocks | Columnar | No | Yes | Yes (Block) | Yes (Block) |
| Avro | Binary | Blocks | Row (Record) | No | Yes | Yes (Block) | Yes (Block) |

Notes:
- SequenceFiles store each row as a key/value pair.
  - The *key* type and its value is ignored.
  - The *value* type **must** be set to the Hadoop `Text` class.
  - The content of the value field is treated as text.
  - Supported serialization formats are text, CSV, and TSV.
- Optimized RCFiles (ORC) are not supported at all.

### Does Okera support any user-defined functions (UDFs)?

Okera supports [Hive's UDFs](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF).

### Does Okera use YARN?

Okera does not use YARN.
The Okera components are deployed on dedicated servers using Docker containers and use Kubernetes under the hood to manage the distributed system deployment.

### How does Okera route front-end requests?

For traffic routing, all _non_-worker services run behind a load balancer.
This means you can call the REST server, planner, or catalog via any hostname or IP on the cluster and the request will get routed to the right place.

To enable Okera to carefully control the load and efficiency on the data path, the worker services are not load balanced.
This is why, for example, you can use the REST server CNAME for the planner.
The CNAME resolves to an IP on the cluster, and then gets routed through Okera as necessary.

### How does Okera solve the small-files problem?

There are a few problems with small files, both from the storage side and the compute side.
For storage, small files lead to more metadata overhead gumming up the works.
This is particularly problematic for HDFS NameNode.
For most Hadoop compute frameworks, small files can cause the computation to be much slower due to the high per file/per task overhead.

For compute, ODAS will combine small files automatically which often provides significant performance benefits. Okera does not currently manage writing files so does not directly help with the issue on the storage side. For non-HDFS storage managers, this may not be an issue at all. Users will want to do background compaction/ETL to combine small files. Okera does help with this illicitly by decoupling applications reading the data from the storage (i.e. file path details).

### Can Okera Catalog span multiple data centers?

The Catalog itself cannot span multiple data centers.
The data backing is stored in MySQL (or Aurora on AWS), and we recommend replication of the backing DB.
The contents in the Catalog can span multiple clusters (For example, datasets that are from different data centers).

### Does Okera support any type of data indexing?

There are no plans to support indices in the short or medium term.
Some of the use cases indexing enables can be solved using good partitioning schemes and enabling certain features when writing parquet.

### Does Okera guarantee backward compatibility with Amazon EMR?

New versions of ODAS will maintain compatibility with EMR client jars.
Functionality introduced in newer versions of ODAS may not work with old EMR client jars, but none of the existing functionality will break as a result of a ODAS upgrade.

## Usage Questions

### Are there differences between MSCK REPAIR TABLE in Hive vs ALTER TABLE RECOVER PARTITIONS in ODAS?

`ALTER TABLE RECOVER PARTITIONS` is effectively an alias for `MSCK REPAIR TABLE`.
The primary difference in usability is that `ALTER TABLE RECOVER PARTITIONS` works from Okera's database CLI utility (odb) or through the REST API.
The `MSCK REPAIR TABLE` command must be run directly from Hive.
Hive then translates the `MSCK` call to `ALTER TABLE RECOVER PARTITIONS` and distributes it to the planner as a call to each partition.
The overhead of this translation and distribution results in slower performance from Hive vs. natively through ODAS.

### What exactly is granting ALL on a URI needed for?

Additional related questions are:

- Is it only needed when creating tables?
- Does a user need permissions on a URI to add partitions or not?

Consider a common scenario where users are allowed to add partitions to an existing table, but otherwise only have read access (that is, the `SELECT` privilege) to the same.
There are two variations of the SQL command adding new partitions:

```sql
ALTER TABLE db.tb ADD PARTITION (part=1);
ALTER TABLE db.tb ADD PARTITION (part=1) LOCATION 's3://....';
```

The general rule is this: The URI is checked anytime it is *explicitly* used in the DDL command, typically with the `LOCATION` keyword in the query.
This happens most commonly in `CREATE EXTERNAL TABLE .. LOCATION 's3://...'`, but also for registering UDFs, or `ALTER TABLE`, `ADD PARTITION`, and so on.
The creator must have URI access to that path (or a prefix thereof).
This is useful, for example, if you want the user to only be able to create tables over their bucket in S3.

This is not applicable if only admins create tables.
Since they are admins, they are allowed to reference all locations in the DDL statements (and therefore are only subjected to the access rights of our servers).
In other words, there is no reason to grant permissions on URIs if the admin creates all the tables.

Regarding the two examples given, the URI permissions are checked on the second statement only.
For the first statement the path is derived implicitly from the table's base table path and not checked again.

More on this topic can be found in the [Authorization](authorization) documentation.

### What is the largest scale Okera has run at?

Okera has scaled up to 800 nodes on a single cluster as well supported a federated deployment with multiple clusters that share metadata.
In addition, we scale test over 200K tables/views and single datasets that are in the hundreds of TBs.

### Is Okera case sensitive?

Okera is not case sensitive.  
It supports upper or lower case names and everything translates to lower case.  
However, where Okera integrates with other systems it adapts their case sensitivity if any.
For example, user groups coming from Active Directory are not case sensitive but Unix groups are.
Okera has no plans to support case sensitivity.

### Does Okera support table statistics?

The subset of SQL that Okera supports and are run in ODAP don't benefit much from statistics so they are not required.
Statistics can be very important for frameworks integrated with Okera (e.g. Spark or Hive) and the Okera catalog supports the exact Hive metastore table statistics and they can be read and set the same way (`describe formatted` and `alter table`).
Specifically the statistics that are available are: `numFiles`, `rawDataSize`, `totalSize` and `rowCount`.

### Implicit Conversions- Datatype not comparable?

**Problem:** Getting the following error while creating a view:

`"Could not execute ddl., detail:AnalysisException: operands of type BIGINT and STRING are not comparable: "Datatype not comparable"`

**Answer:** Okera does not allow implicit conversions that may be unsafe.

In the above error, a user needs to either:

* Change the STRING column to be BIGINT if possible
* Or use explicit conversion by explicitly forcing a cast from string to big int.
	* Such as: `e.event_id=er.event_id  --> e.event_id = CAST(er.event_id as BIGINT)
`

### Lateral Views in Okera

Okera does not have native support for Hive lateral views and those views need to be executed outside of Okera using External Views.

> Please note:
>
> - Since this view is executed outside of Okera, the user must have the prerequisite privileges on the underlying tables that make up the Lateral View.
> - These steps also assume you have a common HMS database being used by Okera enabled, and non-Okera enabled Hive/EMR clusters.

You can take the following simple steps to make use a lateral view as an external Okera view:

1. Start up an EMR cluster without ODAS enabled.
2. Use this non-Okera EMR cluster to create the lateral view.
3. Using DbCli or the WebUI workspace, define this view as an external view as:

```sql
alter view okera_views.lateral_view_470 set tblproperties('cerebro.external.view'='true');
```

Start a Okera enabled EMR cluster, and use the view.

### Is creating a table using XML SerDes supported?

Yes, ODAS supports a subset of valid Hive SerDes that meet the following criteria:

- The SerDes must use the `text` serialization which means that this file format consists of line by line text, with some arbitrary serialization for each line.
- Okera requires SerDes that work on data that is single-line records. It looks at the SerDes that are being referenced to validate that it can run in this mode. If not, there may be other XML SerDes that support single-line input. For more details, please refer to our documentation [here](https://docs.okera.com/extending-odas#using-it-from-hive-emr)

### Is time to deploy dependent on cluster size?

**Question:**

A 15 node cluster takes much longer to deploy than a 2 node cluster. Are nodes created sequentially? If not, why does it seem it takes longer the larger the cluster?

**Answer:**

There is some additional overhead per node in a cluster stemming from us synchronously launching each EC2 instance and waiting until AWS gives us the EC2 instance metadata before launching the next EC2 instance.

After the instance is launched, the remaining steps (installing binaries, starting services, etc) happen in parallel.
Typically, assuming the EC2 machines are available in your region, the EC2 launch time is small.

The forthcoming support for running ODAS on AWS AutoscalingGroups (ASGs) should minimize the per-node cost as ASGs spin up mostly in parallel.

### Does a schema order matter for parquet files?

**Question:**

Does schema order matter for Parquet files?

**Answer:**

Yes. ODAS resolves Parquet schemas by ordinal by default so the order in the file needs to match the order defined in the table schema.

### Recover vs Add Partitions

**Question:**

- Will using `recover partitions` once per day for ~100 datasets (and expanding) work better or worse than `add partition`?
- What does `recover partitions` actually do behind the scenes?

**Recommendation:**

- Okera has optimized the Recover Partitions operation. 
It should be okay for the size of data mentioned above.

The Add Partition operation will always be faster than the Recover Partitions operation because Recover Partitions requires scanning the existing dataset to discover new partitions.

Functionally, it is safer to rely on Recover Partitions in that if there is a bug in your script that inadvertently skips a partition, then Recover Partitions would pick it up.

- Behind the scenes, Recover Partitions analyzes the physical path and the HMS partition metadata and adds just the missing partitions. You can certainly optimize this by calling `add partitions` directly if you know which physical paths are new (without inspecting the physical paths). If you're inspecting the physical paths to determine what data is new, then the suggestion is to use `recover partitions` to keep things simple. If the end-to-end wall clock timing of the 100 `recover partitions` calls is taking too long, then parallelize those calls as much as possible.

### Auto Partition Recovery

**Question:**

Can Okera automatically recovery partitions or do I have to run `add partition` or `recover partitions`?

**Answer:**

Starting in version 1.2.0, Okera will automatically, in the background, recover partitions for tables that follow the standard physical layout (files are in subdirectories with the partition values in the path).
This means that these partitions will be eventually consistent and have the correct partition metadata.
In the default configuration, this is expected to happen after, at most, a few minutes.

A few things to note:

* Pipelines that are integrated with the Metastore (`insert into` using Hive or Spark for example) should see no change in behavior.
Since they are integrated, upon completion, the metadata is updated.

* This functionality is *not* suitable for pipelines that have dependencies in the pipeline and are not integrated with the Metastore.
For example, if a pipeline wrote to a new partition without using the Metastore APIs (e.g. aws s3 cp) and then immediately tried to read the new partition, there is no guarantee the background recovery has completed when the read happens.
These jobs still need to manually include `add partition` or `recover partition` between the write and read.

* The recovery is only triggered for tables that have been accessed (data or metadata) recently.
The rationale for this is not to burn resources recovering very stale tables, which is common in large catalogs.

* Partitions that are not part of the tables subdirectory are not auto recovered.

### Granting on URI

**What exactly is granting all on a URI needed for?**

What exactly is granting all on a URI needed for? Is it only needed when creating tables?
Because, if an admin create tables on behalf of all users so it already has all privileges on everything.
Does a user need permissions on a URI to add partitions or not?

The only command we allow our users to do (besides reading data) is adding partitions.
That command works with and without specifying the LOCATION keyword.

```sql
ALTER TABLE db.tb ADD PARTITION (part=1)
ALTER TABLE db.tb ADD PARTITION (part=1) LOCATION 's3://....'
```

Are the URI permissions only checked on the second command?

**Answer:**

The URI is checked anytime it is explicitly used in the DDL command, typically with the 'LOCATION' keyword in the query.
This happens most commonly in `CREATE EXTERNAL TABLE .. LOCATION s3://...` but also for register UDFs or alter table, add partition, etc.
The creator must have URI access to that path (or a prefix of).
This is useful for example, if you want the user to only be able to prevent tables over their bucket in s3.

This is not applicable if only admins create tables.
Since they are admins, they are allowed to reference all locations in the DDL statements (and therefore only subject to the access rights of our servers).  
Hence, there is not any reason to grant permissions on URIs if the admin creates all the tables.

Regarding the two examples given, the URI permissions are checked on the second statement only.
For the first statement the path is derived implicitly from the table base table path and not checked.
